<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Adaptivetracking.github.io : adaptive tracking" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Adaptive tracking</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/adaptivetracking/AdaptiveTracking">View on GitHub</a>

          <h1 id="project_title">Adaptive tracking</h1>
          <h2 id="project_tagline">Fusion of tracking techniques to enhance adaptive real-time tracking of arbitrary objects</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <p>This is the homepage of our IHCI 2014 paper <em>"Fusion of tracking techniques to enhance adaptive real-time tracking of arbitrary objects"</em>, which presents an algorithm for tracking arbitrary objects and learning their appearance on-the-fly. The tracking starts with a single annotated frame, where a bounding box around the target is given. The tracking algorithm then computes a new bounding box around the target for each of the following frames and is able to re-detect it after an occlusion. On this website, you can find the code, configurations, and detailled benchmark results that were reported in the paper.</p>

        <p>To give an example of what the tracking algorithm should be able to do, we show four frames of sequence F of the <a href="http://www.iai.uni-bonn.de/~kleind/tracking/">Bonn Benchmark on Tracking</a>. The blue bounding box is the output of our tracker.</p>

        <p style="text-align: center;"><img src="images/frame0.png" /> <img src="images/frame130.png" /> <img src="images/frame147.png" /> <img src="images/frame156.png" /></p>

        <p><strong>First image:</strong> Tracking is initialized by the ground truth. <strong>Second image:</strong> The algorithm tracks the position of the person. <strong>Third image:</strong> There is no bounding box because the person is not visible anymore. <strong>Fourth image:</strong> The person is re-detected as soon as it becomes visible again.</p>

        <h3>Code</h3>

          <p>You can find the source code of the tracker in <a href="https://github.com/adaptivetracking/AdaptiveTracking">our GitHub repository</a>.</p>

        <h3>Configurations</h3>

          <p>The following <a href="downloads/configs.zip">configurations files</a> are necessary to confirm our benchmark results:</p>

          <ul>
            <li><strong>bobot.cfg</strong> Image sequences with ground truth using the <a href="http://www.iai.uni-bonn.de/~kleind/tracking/">BoBoT dataset</a> (paths may need adjustment)</li>
            <li><strong>BT.cfg</strong> Baseline tracker</li>
            <li><strong>OF.cfg</strong> Tracker with optical-flow-based motion model</li>
            <li><strong>LC.cfg</strong> Tracker with learning condition</li>
            <li><strong>OF-LC.cfg</strong> Tracker with optical-flow-based motion model and learning condition</li>
            <li><strong>OF-LC-SW.cfg</strong> Tracker with optical-flow-based motion model, learning condition, and sliding-window-based measurement model</li>
          </ul>

        <h3>Benchmark results</h3>

          <p>The <a href="downloads/bobot-results.zip">detailed tracking output</a> of the evaluation contains a directory for each tested tracker variation:</p>

          <ul>
            <li><strong>BT</strong> Baseline tracker</li>
            <li><strong>OF</strong> Tracker with optical-flow-based motion model</li>
            <li><strong>LC</strong> Tracker with learning condition</li>
            <li><strong>OF-LC</strong> Tracker with optical-flow-based motion model and learning condition</li>
            <li><strong>OF-LC-SW</strong> Tracker with optical-flow-based motion model, learning condition, and sliding-window-based measurement model</li>
          </ul>

          <p>Insides those directories, there are further directories for each of the BoBoT image sequences. Within those, there are the following files:</p>

          <ul>
            <li><strong>run#</strong> Tracking output (in BoBoT format) of run # (0-19)</li>
            <li><strong>overlap#</strong> Overlap ratios of run # (0-19)</li>
          </ul>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a>, last update on October 08, 2014</p>
      </footer>
    </div>

    

  </body>
</html>
