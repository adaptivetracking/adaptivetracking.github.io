<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Adaptivetracking.github.io : adaptive tracking" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Adaptive tracking</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/adaptivetracking/AdaptiveTracking">View on GitHub</a>

          <h1 id="project_title">Adaptive tracking</h1>
          <h2 id="project_tagline">Fusion of tracking techniques to enhance adaptive real-time tracking of arbitrary objects</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <p>This is the homepage of our IHCI 2014 paper <em>"Fusion of tracking techniques to enhance adaptive real-time tracking of arbitrary objects"</em>, which presents an algorithm for tracking arbitrary objects and learning their appearance on-the-fly. The tracking starts with a single annotated frame, where a bounding box around the target is given. The tracking algorithm then computes a new bounding box around the target for each of the following frames and is able to re-detect it after an occlusion. On this website, you can find the code, configurations, and detailled benchmark results that were reported in the paper.</p>

        <p>To give an example of what the tracking algorithm should be able to do, we show four frames of sequence F of the <a href="http://www.iai.uni-bonn.de/~kleind/tracking/">Bonn Benchmark on Tracking</a>. The blue bounding box is the output of our tracker.</p>

        <p style="text-align: center;"><img src="images/frame0.png" /> <img src="images/frame130.png" /> <img src="images/frame147.png" /> <img src="images/frame156.png" /></p>

        <p><strong>First image:</strong> Tracking is initialized by the ground truth. <strong>Second image:</strong> The algorithm tracks the position of the person. <strong>Third image:</strong> There is no bounding box because the person is not visible anymore. <strong>Fourth image:</strong> The person is re-detected as soon as it becomes visible again.</p>

        <h3>Code</h3>

          <p>You can find the source code of the tracker in <a href="https://github.com/adaptivetracking/AdaptiveTracking">our GitHub repository</a>. In order to verify our results, you need to make sure to checkout the tag <a href="https://github.com/adaptivetracking/AdaptiveTracking/tree/IHCI2014">IHCI2014</a>, as the following commits changed the tracking performance slightly. We had bugs in the extended HOG feature extraction, whose fixes affect the benchmark results.</p>

        <h3>Configurations</h3>

          <p>The following <a href="downloads/configs.zip">configurations files</a> are necessary to confirm our benchmark results:</p>

          <ul>
            <li><strong>bobot.cfg</strong> Image sequences with ground truth using the <a href="http://www.iai.uni-bonn.de/~kleind/tracking/">BoBoT dataset</a> (paths may need adjustment)</li>
            <li><strong>BT.cfg</strong> Baseline tracker</li>
            <li><strong>OF.cfg</strong> Tracker with optical-flow-based motion model</li>
            <li><strong>LC.cfg</strong> Tracker with learning condition</li>
            <li><strong>OF-LC.cfg</strong> Tracker with optical-flow-based motion model and learning condition</li>
            <li><strong>OF-LC-SW.cfg</strong> Tracker with optical-flow-based motion model, learning condition, and sliding-window-based measurement model</li>
          </ul>

        <h3>Benchmark results</h3>

          <p>The <a href="downloads/bobot-results.zip">detailed tracking output</a> of the evaluation contains a directory for each tested tracker variation:</p>

          <ul>
            <li><strong>BT</strong> Baseline tracker</li>
            <li><strong>OF</strong> Tracker with optical-flow-based motion model</li>
            <li><strong>LC</strong> Tracker with learning condition</li>
            <li><strong>OF-LC</strong> Tracker with optical-flow-based motion model and learning condition</li>
            <li><strong>OF-LC-SW</strong> Tracker with optical-flow-based motion model, learning condition, and sliding-window-based measurement model</li>
          </ul>

          <p>Insides those directories, there are further directories for each of the BoBoT image sequences. Within those, there are the following files:</p>

          <ul>
            <li><strong>run#</strong> Tracking output (in BoBoT format) of run # (0-19)</li>
            <li><strong>overlap#</strong> Overlap ratios of run # (0-19)</li>
          </ul>

        <h3>Limitations</h3>

          <p>The tracking algorithm is far from perfect and has its limitations. For example, the used linear SVM and HOG features capture the shape of the target and therefore allow tracking under out-of-plane rotations (as long as the contour does not change as much), but other details, that may be necessary to discriminate the target from similar objects, might get lost. If the shape does change drastically, as is the case with deformations (e.g. a gymnast during performance), the tracking usually fails. Furthermore, the resolution of the image sequences is important, as images below 320x240 pixels may be too small to capture information, while images above might need other HOG parameters (cell size of 8 instead of 5, more cells per target patch) and another minimum target size.</p>

          <p>Further limitations:</p>

          <ul>
            <li>Sudden appearance changes (especially right in the beginning)</li>
            <li>Objects similar in appearance to the target</li>
            <li>Deformations</li>
            <li>Very low resolution (below 320x240 pixels) or image quality</li>
          </ul>

        <h3>Paper &amp; Poster</h3>

          <p>Paper and poster were published at the international conference on Intelligent Human Computer Interaction (IHCI) 2014 in Ã‰vry (near Paris), France.</p>

          <ul>
            <li><a href="downloads/ihci2014-paper.pdf">IHCI 2014 Paper</a></li>
            <li><a href="downloads/ihci2014-poster.pdf">IHCI 2014 Poster</a></li>
          </ul>


      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a>, last update on March 09, 2016</p>
      </footer>
    </div>

    

  </body>
</html>
